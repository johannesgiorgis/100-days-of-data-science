# 100 Days Of Data Science - Log

### Day 1: Wednesday, March 20th, 2019

**Today's Progress**: I worked through _Chi-squared tests_ and _Multi category chi-squared tests_ missions within DataQuest's Data Scientist _Probability and Statistics_ step.

**Thoughts**: After working through implementing the chi-square by hand (not necessarily the most complicated), I was provided with the correct function to use [`scipy.stats.chisquare`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.chisquare.html). This reminded me of a statement I keep hearing repeatedly - we don't necessarily need to know the full details of the various algorithms to use them thanks to libraries like `scipy` - we just need to know WHEN and HOW to use them correctly.

**Link(s) to work**: No links. But will be starting on a guided project tomorrow which will utilize my learnings from today.

### Day 2: Thursday, March 21st, 2019

**Today's Progress**: I completed the _Guided Project: Winning Jeopardy_ where I applied the Chi-Squared test to come up with a winning strategy for Jeopardy.

**Thoughts**: It was interesting to see the application of Chi-Squared test in a real world setting. No breakthroughs in winning Jeopardies but definitely starting to see and understand where this tool can be used. I'll need to do some more reading, apply it to my own project and write a blog about it.

**Link(s) to work**: 
1. [Winning Jeopardy Notebook](https://nbviewer.jupyter.org/github/johannesgiorgis/dataquest/blob/master/data_science_path/projects/project16_winning_jeopardy/project16_winning_jeopardy.ipynb)

### Day 3: Friday, March 22nd, 2019

**Today's Progress**: After a false start on DataQuest's Data Engineering path (_Introduction to Postgres_) due to some technical issues with the website, I worked through the _Introduction to Spark_ mission of the Data Scientist path. I installed and set up Spark on my computer and verified PySpark worked via CLI and Jupyter Notebook.

**Thoughts**: It was great revising my Spark knowledge and getting the chance to use PySpark again. Also `findspark` is a nifty python package for finding the SPARK_HOME environment variable without you having to explicitly call it in your scripts. I look forward to diving deeper into Spark and building applications and pipelines with it.

**Link(s) to work**: 
1. [Verify Spark Installation via Jupyter Notebook](https://nbviewer.jupyter.org/github/johannesgiorgis/dataquest/blob/master/data_science_path/verify_spark_installation/test_spark_installation.ipynb)

### Day 4: Saturday, March 23rd, 2019

**Today's Progress**: I finished up DataQuest's _Spark and Map-Reduce_ mission.

**Thoughts**: The course was a good overview of Spark's capabilities, covering transformations and actions, Spark DataFrames and Spark SQL. I'll plan on getting some more hands on experience by using Spark on some larger datasets as part of a Data Science Challenge.

**Link(s) to work**: 
1. [DataQuest's Spark & MapReduce Mission](https://app.dataquest.io/course/spark-map-reduce)